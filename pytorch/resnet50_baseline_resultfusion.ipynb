{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline result fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hiddenlayer as hl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD,Adam\n",
    "import torch.utils.data as Data\n",
    "from torchvision import models\n",
    "from  torchvision import transforms\n",
    "from  torchvision.datasets import ImageFolder\n",
    "import pickle as pkl\n",
    "import torchvision.models as models\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneResnet50(nn.Module):\n",
    "    def __init__(self, num_class=3):\n",
    "        super(FineTuneResnet50, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        resnet50_net_MRI = models.resnet50(pretrained=True)\n",
    "        resnet50_net_PET = models.resnet50(pretrained=True)\n",
    "        self.features_MRI = nn.Sequential(*list(resnet50_net_MRI.children())[:-1])\n",
    "        self.features_PET = nn.Sequential(*list(resnet50_net_PET.children())[:-1])\n",
    "        self.fc_comb = nn.Sequential(\n",
    "            nn.Linear(4096,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(128,3)\n",
    "        )\n",
    "        \n",
    "         \n",
    "    def MA(self, x, label):\n",
    "        # x (k, v), label (q)\n",
    "        B, C_kv = x.shape\n",
    "        B, C_q = label.shape\n",
    "        self.kv = nn.Linear(C_kv, C_kv * 3 * 2).cuda()\n",
    "        self.q = nn.Linear(C_q, C_kv * 3).cuda()\n",
    "        self.at_fx = nn.Linear(C_kv * 3, C_kv).cuda()\n",
    "        #self.ffn = nn.Linear(C_kv, C_kv).cuda()\n",
    "        kv = self.kv(x).reshape(2, B, 3, C_kv)\n",
    "        k, v = kv[0], kv[1]\n",
    "        q = self.q(label).reshape(B, 3, C_kv)\n",
    "        attn = torch.einsum(\"bhq,bhk->bhqk\", [q, k])\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x_ = torch.einsum(\"bhqk,bhk->bhq\", [attn, v])\n",
    "        x_ = x_.reshape(B, C_kv * 3)\n",
    "        x = self.at_fx(x_) + x\n",
    "        #x = self.ffn(x) + x\n",
    "        return x    \n",
    "        \n",
    "\n",
    " \n",
    "    def forward(self, MRI,PET):\n",
    "        MRI = self.features_MRI(MRI)\n",
    "        PET = self.features_PET(PET)\n",
    "        MRI = MRI.view(MRI.size(0),-1)\n",
    "        PET = PET.view(PET.size(0),-1)# 将第二次卷积的输出拉伸为一行\n",
    "        \n",
    "        # MRI_ma = self.MA(MRI,PET)\n",
    "        # PET_ma = self.MA(PET,MRI)\n",
    "        # concat = torch.cat((MRI_ma, PET_ma), 1)\n",
    "        concat = torch.cat((MRI, PET), 1)\n",
    "        output = self.fc_comb(concat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyResnet = FineTuneResnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneResnet50(\n",
       "  (features_MRI): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (features_PET): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc_comb): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义优化器\n",
    "optimizer = torch.optim.Adam(MyResnet.parameters(),lr=0.00001,weight_decay=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()#损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#记录训练过程指标\n",
    "historyl = hl.History()\n",
    "#使用Canves进行可视化\n",
    "\n",
    "canvasl = hl.Canvas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        super(MyDataset, self).__init__()\n",
    "        MRI_PET_match_all = pkl.load(open(root,\"rb\"),encoding='iso-8859-1')\n",
    "        MRI = []\n",
    "        PET = []\n",
    "        group = []\n",
    "        for index,row in MRI_PET_match_all.iterrows():\n",
    "            MRI.append(row['MRI_img_array'])\n",
    "            PET.append(row['PET_img_array'])\n",
    "            group_ = torch.tensor(row['Group'],dtype=torch.float)\n",
    "            group.append(group_)\n",
    "        self.MRI = MRI\n",
    "        self.PET = PET\n",
    "        self.group = group  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mri =torch.from_numpy(self.MRI[index].transpose([2,0,1])).float().to(DEVICE)\n",
    "        pet = torch.from_numpy(self.PET[index].transpose([2,0,1])).float().to(DEVICE)\n",
    "        group = self.group[index].to(DEVICE)\n",
    "        return mri,pet,group\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.MRI)\n",
    "\n",
    "train_data = MyDataset(\"/home/gc/gechang/gec_multi_fusion/end_to_end/train.pkl\")\n",
    "test_data = MyDataset(\"/home/gc/gechang/gec_multi_fusion/end_to_end/test.pkl\")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch: 0 train_loss: 0.9875878229494226 train_acc: tensor(0.5659, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 0 val_loss: 0.9645949482917786 val_acc: tensor(0.5450, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 1 train_loss: 0.867944040591624 train_acc: tensor(0.5885, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 1 val_loss: 0.8966050791740418 val_acc: tensor(0.5550, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 2 train_loss: 0.6879340793138167 train_acc: tensor(0.7127, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 2 val_loss: 0.7514115631580353 val_acc: tensor(0.6150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 3 train_loss: 0.5001013025061848 train_acc: tensor(0.8231, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 3 val_loss: 0.6873547613620759 val_acc: tensor(0.6450, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 4 train_loss: 0.3727361566598622 train_acc: tensor(0.8595, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 4 val_loss: 0.6004386377334595 val_acc: tensor(0.7100, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 5 train_loss: 0.2939439278698925 train_acc: tensor(0.9072, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 5 val_loss: 0.5769381839036941 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 6 train_loss: 0.27443826051809855 train_acc: tensor(0.9235, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 6 val_loss: 0.5876937752962113 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 7 train_loss: 0.21289683814853466 train_acc: tensor(0.9598, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 7 val_loss: 0.5279523953795433 val_acc: tensor(0.8300, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 8 train_loss: 0.1522830133717811 train_acc: tensor(0.9799, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 8 val_loss: 0.5198005966842174 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 9 train_loss: 0.11452271817357208 train_acc: tensor(0.9875, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 9 val_loss: 0.55190783880651 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 10 train_loss: 0.10583817791497543 train_acc: tensor(0.9837, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 10 val_loss: 0.5164306737482548 val_acc: tensor(0.8100, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 11 train_loss: 0.1107381001404642 train_acc: tensor(0.9762, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 11 val_loss: 0.601013929322362 val_acc: tensor(0.7650, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 12 train_loss: 0.113541887285494 train_acc: tensor(0.9699, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 12 val_loss: 0.5555445001646876 val_acc: tensor(0.7900, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 13 train_loss: 0.1000078625303963 train_acc: tensor(0.9849, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 13 val_loss: 0.6386656036973 val_acc: tensor(0.7800, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 14 train_loss: 0.10171623656542568 train_acc: tensor(0.9711, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 14 val_loss: 0.5532319698482752 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 15 train_loss: 0.09657460070651029 train_acc: tensor(0.9724, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 15 val_loss: 0.5843196625821292 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 16 train_loss: 0.05575826961355496 train_acc: tensor(0.9925, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 16 val_loss: 0.5791078848019242 val_acc: tensor(0.8200, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 17 train_loss: 0.058912239604193714 train_acc: tensor(0.9875, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 17 val_loss: 0.6775543437711895 val_acc: tensor(0.7700, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 18 train_loss: 0.060081824368167405 train_acc: tensor(0.9849, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 18 val_loss: 0.6343790616514161 val_acc: tensor(0.7800, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 19 train_loss: 0.11855264032584437 train_acc: tensor(0.9749, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 19 val_loss: 0.5505118566937744 val_acc: tensor(0.8300, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 20 train_loss: 0.06819126583925604 train_acc: tensor(0.9824, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 20 val_loss: 0.5953248170344159 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 21 train_loss: 0.04830201947535744 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 21 val_loss: 0.6371009551687166 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 22 train_loss: 0.026465266330665448 train_acc: tensor(0.9987, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 22 val_loss: 0.7035125013953075 val_acc: tensor(0.7850, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 23 train_loss: 0.07653094437954903 train_acc: tensor(0.9837, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 23 val_loss: 0.6845865816995501 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 24 train_loss: 0.04367794472621368 train_acc: tensor(0.9912, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 24 val_loss: 0.5741241523367353 val_acc: tensor(0.8300, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 25 train_loss: 0.04184470409959779 train_acc: tensor(0.9875, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 25 val_loss: 0.624760370333679 val_acc: tensor(0.8150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 26 train_loss: 0.038619004137384026 train_acc: tensor(0.9912, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 26 val_loss: 0.6470903777983039 val_acc: tensor(0.8150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 27 train_loss: 0.053810271416373616 train_acc: tensor(0.9812, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 27 val_loss: 0.6079012123122811 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 28 train_loss: 0.05699293789602208 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 28 val_loss: 0.6209243786265142 val_acc: tensor(0.8150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 29 train_loss: 0.06657405308280678 train_acc: tensor(0.9812, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 29 val_loss: 0.6580824225023388 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 30 train_loss: 0.04656010874812322 train_acc: tensor(0.9849, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 30 val_loss: 0.6451120275165886 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 31 train_loss: 0.023892070418293852 train_acc: tensor(0.9937, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 31 val_loss: 0.6437365807266906 val_acc: tensor(0.7800, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 32 train_loss: 0.012855363558099103 train_acc: tensor(1.0000, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 32 val_loss: 0.6960066334484145 val_acc: tensor(0.7900, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 33 train_loss: 0.024094194103700752 train_acc: tensor(0.9950, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 33 val_loss: 0.6752974962349981 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 34 train_loss: 0.018396429706439656 train_acc: tensor(0.9975, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 34 val_loss: 0.616234415709041 val_acc: tensor(0.8250, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 35 train_loss: 0.014077856080662669 train_acc: tensor(0.9987, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 35 val_loss: 0.6392307404102757 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 36 train_loss: 0.04189884406017838 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 36 val_loss: 0.7294074268266558 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 37 train_loss: 0.05405045197540462 train_acc: tensor(0.9862, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 37 val_loss: 0.6870363255217672 val_acc: tensor(0.7900, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 38 train_loss: 0.061306717687542346 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 38 val_loss: 0.6321379308309406 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 39 train_loss: 0.022348539228930864 train_acc: tensor(0.9950, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 39 val_loss: 0.8011620261543431 val_acc: tensor(0.8200, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#对模型进行迭代训练，对所有的数据训练epoch轮\n",
    "for epoch in range(40):\n",
    "    train_loss_epoch = 0\n",
    "    val_loss_epoch = 0\n",
    "    train_corrects = 0\n",
    "    val_corrects = 0\n",
    "    #对训练数据的加载器进行迭代计算\n",
    "    MyResnet.train().cuda()\n",
    "    for step,(mri,pet,group) in enumerate(train_loader):\n",
    "        ##计算每个batch的损失\n",
    "        output = MyResnet(mri,pet)\n",
    "        loss = loss_func(output,group.long())#交叉熵损失函数\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        optimizer.zero_grad()#每个迭代步的梯度初始化为0\n",
    "        loss.backward()#损失的后向传播，计算梯度\n",
    "        optimizer.step()#使用梯度进行优化\n",
    "        train_loss_epoch += loss.item()*group.size(0)\n",
    "        train_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "    #计算一个epoch的损失和精度\n",
    "    train_loss = train_loss_epoch/len(train_data.group)\n",
    "    train_acc = train_corrects.double()/len(train_data.group)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss,\"train_acc:\",train_acc)\n",
    "     #计算在验证集上的表现\n",
    "    MyResnet.eval()\n",
    "    for step,(mri,pet,group) in enumerate(test_loader):\n",
    "        output = MyResnet(mri,pet)\n",
    "        loss = loss_func(output,group.long())\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        val_loss_epoch += loss.item()*group.size(0)\n",
    "        val_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "\n",
    "    #计算一个epoch上的输出loss和acc\n",
    "    val_loss = val_loss_epoch/len(test_data.group)\n",
    "    val_acc = val_corrects.double()/len(test_data.group)\n",
    "    print(\"epoch:\",epoch,\"val_loss:\",val_loss,\"val_acc:\",val_acc)\n",
    "    #保存每个epoch上的输出loss和acc\n",
    "    historyl.log(epoch,train_loss=train_loss,val_loss = val_loss,train_acc = train_acc.item(),val_acc = val_acc.item())\n",
    "    #可视化网络训练的过程\n",
    "    # with canvasl:\n",
    "    #     canvasl.draw_plot([historyl[\"train_loss\"],historyl[\"val_loss\"]])\n",
    "    #     canvasl.draw_plot([historyl[\"train_acc\"],historyl[\"val_acc\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch: 0 train_loss: 0.9875878229494226 train_acc: tensor(0.5659, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 0 val_loss: 0.9645949482917786 val_acc: tensor(0.5450, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 1 train_loss: 0.867944040591624 train_acc: tensor(0.5885, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 1 val_loss: 0.8966050791740418 val_acc: tensor(0.5550, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 2 train_loss: 0.6879340793138167 train_acc: tensor(0.7127, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 2 val_loss: 0.7514115631580353 val_acc: tensor(0.6150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 3 train_loss: 0.5001013025061848 train_acc: tensor(0.8231, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 3 val_loss: 0.6873547613620759 val_acc: tensor(0.6450, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 4 train_loss: 0.3727361566598622 train_acc: tensor(0.8595, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 4 val_loss: 0.6004386377334595 val_acc: tensor(0.7100, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 5 train_loss: 0.2939439278698925 train_acc: tensor(0.9072, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 5 val_loss: 0.5769381839036941 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 6 train_loss: 0.27443826051809855 train_acc: tensor(0.9235, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 6 val_loss: 0.5876937752962113 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 7 train_loss: 0.21289683814853466 train_acc: tensor(0.9598, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 7 val_loss: 0.5279523953795433 val_acc: tensor(0.8300, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 8 train_loss: 0.1522830133717811 train_acc: tensor(0.9799, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 8 val_loss: 0.5198005966842174 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 9 train_loss: 0.11452271817357208 train_acc: tensor(0.9875, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 9 val_loss: 0.55190783880651 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 10 train_loss: 0.10583817791497543 train_acc: tensor(0.9837, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 10 val_loss: 0.5164306737482548 val_acc: tensor(0.8100, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 11 train_loss: 0.1107381001404642 train_acc: tensor(0.9762, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 11 val_loss: 0.601013929322362 val_acc: tensor(0.7650, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 12 train_loss: 0.113541887285494 train_acc: tensor(0.9699, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 12 val_loss: 0.5555445001646876 val_acc: tensor(0.7900, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 13 train_loss: 0.1000078625303963 train_acc: tensor(0.9849, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 13 val_loss: 0.6386656036973 val_acc: tensor(0.7800, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 14 train_loss: 0.10171623656542568 train_acc: tensor(0.9711, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 14 val_loss: 0.5532319698482752 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 15 train_loss: 0.09657460070651029 train_acc: tensor(0.9724, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 15 val_loss: 0.5843196625821292 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 16 train_loss: 0.05575826961355496 train_acc: tensor(0.9925, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 16 val_loss: 0.5791078848019242 val_acc: tensor(0.8200, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 17 train_loss: 0.058912239604193714 train_acc: tensor(0.9875, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 17 val_loss: 0.6775543437711895 val_acc: tensor(0.7700, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 18 train_loss: 0.060081824368167405 train_acc: tensor(0.9849, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 18 val_loss: 0.6343790616514161 val_acc: tensor(0.7800, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 19 train_loss: 0.11855264032584437 train_acc: tensor(0.9749, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 19 val_loss: 0.5505118566937744 val_acc: tensor(0.8300, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 20 train_loss: 0.06819126583925604 train_acc: tensor(0.9824, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 20 val_loss: 0.5953248170344159 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 21 train_loss: 0.04830201947535744 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 21 val_loss: 0.6371009551687166 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 22 train_loss: 0.026465266330665448 train_acc: tensor(0.9987, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 22 val_loss: 0.7035125013953075 val_acc: tensor(0.7850, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 23 train_loss: 0.07653094437954903 train_acc: tensor(0.9837, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 23 val_loss: 0.6845865816995501 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 24 train_loss: 0.04367794472621368 train_acc: tensor(0.9912, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 24 val_loss: 0.5741241523367353 val_acc: tensor(0.8300, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 25 train_loss: 0.04184470409959779 train_acc: tensor(0.9875, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 25 val_loss: 0.624760370333679 val_acc: tensor(0.8150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 26 train_loss: 0.038619004137384026 train_acc: tensor(0.9912, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 26 val_loss: 0.6470903777983039 val_acc: tensor(0.8150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 27 train_loss: 0.053810271416373616 train_acc: tensor(0.9812, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 27 val_loss: 0.6079012123122811 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 28 train_loss: 0.05699293789602208 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 28 val_loss: 0.6209243786265142 val_acc: tensor(0.8150, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 29 train_loss: 0.06657405308280678 train_acc: tensor(0.9812, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 29 val_loss: 0.6580824225023388 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 30 train_loss: 0.04656010874812322 train_acc: tensor(0.9849, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 30 val_loss: 0.6451120275165886 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 31 train_loss: 0.023892070418293852 train_acc: tensor(0.9937, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 31 val_loss: 0.6437365807266906 val_acc: tensor(0.7800, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 32 train_loss: 0.012855363558099103 train_acc: tensor(1.0000, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 32 val_loss: 0.6960066334484145 val_acc: tensor(0.7900, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 33 train_loss: 0.024094194103700752 train_acc: tensor(0.9950, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 33 val_loss: 0.6752974962349981 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 34 train_loss: 0.018396429706439656 train_acc: tensor(0.9975, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 34 val_loss: 0.616234415709041 val_acc: tensor(0.8250, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 35 train_loss: 0.014077856080662669 train_acc: tensor(0.9987, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 35 val_loss: 0.6392307404102757 val_acc: tensor(0.8050, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 36 train_loss: 0.04189884406017838 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 36 val_loss: 0.7294074268266558 val_acc: tensor(0.7950, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 37 train_loss: 0.05405045197540462 train_acc: tensor(0.9862, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 37 val_loss: 0.6870363255217672 val_acc: tensor(0.7900, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 38 train_loss: 0.061306717687542346 train_acc: tensor(0.9887, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 38 val_loss: 0.6321379308309406 val_acc: tensor(0.8000, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 39 train_loss: 0.022348539228930864 train_acc: tensor(0.9950, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 39 val_loss: 0.8011620261543431 val_acc: tensor(0.8200, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#对模型进行迭代训练，对所有的数据训练epoch轮\n",
    "for epoch in range(40):\n",
    "    train_loss_epoch = 0\n",
    "    val_loss_epoch = 0\n",
    "    train_corrects = 0\n",
    "    val_corrects = 0\n",
    "    #对训练数据的加载器进行迭代计算\n",
    "    MyResnet.train().cuda()\n",
    "    for step,(mri,pet,group) in enumerate(train_loader):\n",
    "        ##计算每个batch的损失\n",
    "        output = MyResnet(mri,pet)\n",
    "        loss = loss_func(output,group.long())#交叉熵损失函数\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        optimizer.zero_grad()#每个迭代步的梯度初始化为0\n",
    "        loss.backward()#损失的后向传播，计算梯度\n",
    "        optimizer.step()#使用梯度进行优化\n",
    "        train_loss_epoch += loss.item()*group.size(0)\n",
    "        train_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "    #计算一个epoch的损失和精度\n",
    "    train_loss = train_loss_epoch/len(train_data.group)\n",
    "    train_acc = train_corrects.double()/len(train_data.group)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss,\"train_acc:\",train_acc)\n",
    "     #计算在验证集上的表现\n",
    "    MyResnet.eval()\n",
    "    for step,(mri,pet,group) in enumerate(test_loader):\n",
    "        output = MyResnet(mri,pet)\n",
    "        loss = loss_func(output,group.long())\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        val_loss_epoch += loss.item()*group.size(0)\n",
    "        val_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "\n",
    "    #计算一个epoch上的输出loss和acc\n",
    "    val_loss = val_loss_epoch/len(test_data.group)\n",
    "    val_acc = val_corrects.double()/len(test_data.group)\n",
    "    print(\"epoch:\",epoch,\"val_loss:\",val_loss,\"val_acc:\",val_acc)\n",
    "    #保存每个epoch上的输出loss和acc\n",
    "    historyl.log(epoch,train_loss=train_loss,val_loss = val_loss,train_acc = train_acc.item(),val_acc = val_acc.item())\n",
    "    #可视化网络训练的过程\n",
    "    # with canvasl:\n",
    "    #     canvasl.draw_plot([historyl[\"train_loss\"],historyl[\"val_loss\"]])\n",
    "    #     canvasl.draw_plot([historyl[\"train_acc\"],historyl[\"val_acc\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py372')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e0e02a671462d396ecc20af6161881d6338b1e899f7d69a261eb7f1f955476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
