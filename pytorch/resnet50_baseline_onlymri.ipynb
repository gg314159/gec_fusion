{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline only mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hiddenlayer as hl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD,Adam\n",
    "import torch.utils.data as Data\n",
    "from torchvision import models\n",
    "from  torchvision import transforms\n",
    "from  torchvision.datasets import ImageFolder\n",
    "import pickle as pkl\n",
    "import torchvision.models as models\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneResnet50(nn.Module):\n",
    "    def __init__(self, num_class=3):\n",
    "        super(FineTuneResnet50, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        resnet50_net_MRI = models.resnet50(pretrained=True)\n",
    "        self.features_MRI = nn.Sequential(*list(resnet50_net_MRI.children())[:-1])\n",
    "        self.fc_comb = nn.Sequential(\n",
    "            nn.Linear(2048,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(128,3)\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, MRI):\n",
    "        MRI = self.features_MRI(MRI)\n",
    "        \n",
    "        MRI = MRI.view(MRI.size(0),-1)\n",
    "      \n",
    "        output = self.fc_comb(MRI)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyResnet = FineTuneResnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneResnet50(\n",
       "  (features_MRI): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc_comb): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义优化器\n",
    "optimizer = torch.optim.Adam(MyResnet.parameters(),lr=0.00001,weight_decay=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()#损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#记录训练过程指标\n",
    "historyl = hl.History()\n",
    "#使用Canves进行可视化\n",
    "\n",
    "canvasl = hl.Canvas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root,transform_pet = None):\n",
    "        super(MyDataset, self).__init__()\n",
    "        MRI_PET_match_all = pkl.load(open(root,\"rb\"),encoding='iso-8859-1')\n",
    "        MRI = []\n",
    "        PET = []\n",
    "        group = []\n",
    "        for index,row in MRI_PET_match_all.iterrows():\n",
    "            MRI.append(row['MRI_img_array'])\n",
    "            PET.append(row['PET_img_array'])\n",
    "            group_ = torch.tensor(row['Group'],dtype=torch.float)\n",
    "            group.append(group_)\n",
    "        self.MRI = MRI\n",
    "        self.PET = PET\n",
    "        self.group = group  \n",
    "        self.transform_pet = transform_pet\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mri =torch.from_numpy(self.MRI[index].transpose([2,0,1])).float().to(DEVICE)\n",
    "        pet = torch.from_numpy(self.PET[index].transpose([2,0,1])).float().to(DEVICE)\n",
    "        pet = self.transform_pet(pet)\n",
    "        group = self.group[index].to(DEVICE)\n",
    "        return pet,group\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.MRI)\n",
    "\n",
    "\n",
    "train_mean_mri = [4.1620684, 4.1620684, 4.1620684]\n",
    "train_std_mri = [5.2131376, 5.2131376, 5.2131376]\n",
    "train_mean_pet = [4.081158, 4.081158, 4.081158] \n",
    "train_std_pet = [5.1888165, 5.1888165, 5.1888165]\n",
    "\n",
    "test_mean_mri = [4.1623616, 4.1623616, 4.1623616]\n",
    "test_std_mri = [5.2136188, 5.2136188, 5.2136188]\n",
    "test_mean_pet = [4.106387, 4.106387, 4.106387]\n",
    "test_std_pet = [5.18535, 5.18535, 5.18535]\n",
    "    \n",
    "train_transform_mri = transforms.Compose([\n",
    "    transforms.Normalize(train_mean_mri,train_std_mri),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "])\n",
    "\n",
    "train_transform_pet = transforms.Compose([\n",
    "    transforms.Normalize(train_mean_pet,train_std_pet),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "test_transform_mri = transforms.Compose([\n",
    "    \n",
    "    transforms.Normalize(test_mean_mri,test_std_mri),\n",
    "])\n",
    "\n",
    "test_transform_pet = transforms.Compose([\n",
    "   \n",
    "    transforms.Normalize(test_mean_pet,test_std_pet)\n",
    "])\n",
    "\n",
    "\n",
    "train_data = MyDataset(\"/home/gc/gechang/gec_multi_fusion/end_to_end/train_not16.pkl\",transform_pet =train_transform_pet)\n",
    "test_data = MyDataset(\"/home/gc/gechang/gec_multi_fusion/end_to_end/test_not16.pkl\",transform_pet =test_transform_pet)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch: 0 train_loss: 1.0284189592088973 train_acc: tensor(0.4792, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 0 val_loss: 0.9453745469525086 val_acc: tensor(0.5429, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 1 train_loss: 0.834227557012013 train_acc: tensor(0.5996, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 1 val_loss: 0.7908269074277499 val_acc: tensor(0.6558, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 2 train_loss: 0.5143573044027601 train_acc: tensor(0.8322, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 2 val_loss: 0.6459564351620366 val_acc: tensor(0.7391, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 3 train_loss: 0.28945992972169604 train_acc: tensor(0.9125, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 3 val_loss: 0.7333655713209465 val_acc: tensor(0.7332, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 4 train_loss: 0.1707266734114715 train_acc: tensor(0.9480, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 4 val_loss: 0.6290651197239423 val_acc: tensor(0.7920, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 5 train_loss: 0.11533445486000606 train_acc: tensor(0.9656, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 5 val_loss: 0.7081518653179032 val_acc: tensor(0.7817, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 6 train_loss: 0.0882034433872572 train_acc: tensor(0.9740, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 6 val_loss: 0.6772448376993375 val_acc: tensor(0.7985, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 7 train_loss: 0.07438918674098594 train_acc: tensor(0.9779, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 7 val_loss: 0.7331158884442458 val_acc: tensor(0.7998, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 8 train_loss: 0.06202201014916812 train_acc: tensor(0.9816, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 8 val_loss: 0.7977275611306842 val_acc: tensor(0.7960, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 9 train_loss: 0.06575584987178446 train_acc: tensor(0.9800, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 9 val_loss: 0.7016832915039742 val_acc: tensor(0.8063, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 10 train_loss: 0.05398395962880126 train_acc: tensor(0.9845, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 10 val_loss: 0.7277554002160725 val_acc: tensor(0.8100, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 11 train_loss: 0.05157246240414679 train_acc: tensor(0.9863, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 11 val_loss: 0.6394807252582899 val_acc: tensor(0.8206, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 12 train_loss: 0.04734720247731145 train_acc: tensor(0.9863, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 12 val_loss: 0.8001234701157557 val_acc: tensor(0.8106, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 13 train_loss: 0.03950605316287173 train_acc: tensor(0.9882, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 13 val_loss: 0.828258427488386 val_acc: tensor(0.7957, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 14 train_loss: 0.03545558391371742 train_acc: tensor(0.9899, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 14 val_loss: 0.6918283910050396 val_acc: tensor(0.8088, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 15 train_loss: 0.03124701701237687 train_acc: tensor(0.9913, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 15 val_loss: 0.6463753681815814 val_acc: tensor(0.8265, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 16 train_loss: 0.039127881696580775 train_acc: tensor(0.9894, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 16 val_loss: 0.7515123152603808 val_acc: tensor(0.8044, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 17 train_loss: 0.032182424062463855 train_acc: tensor(0.9902, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 17 val_loss: 0.7690188325630253 val_acc: tensor(0.8106, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 18 train_loss: 0.03933490559758086 train_acc: tensor(0.9892, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 18 val_loss: 0.7376445823589998 val_acc: tensor(0.8109, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 19 train_loss: 0.03656940755234765 train_acc: tensor(0.9896, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 19 val_loss: 0.8155599969087863 val_acc: tensor(0.8147, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 20 train_loss: 0.034490971323767945 train_acc: tensor(0.9909, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 20 val_loss: 0.718860555944509 val_acc: tensor(0.8231, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 21 train_loss: 0.026856700217550888 train_acc: tensor(0.9930, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 21 val_loss: 0.7621272771722933 val_acc: tensor(0.8134, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 22 train_loss: 0.03540488885516035 train_acc: tensor(0.9896, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 22 val_loss: 0.693639598585276 val_acc: tensor(0.8159, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 23 train_loss: 0.033323436480547704 train_acc: tensor(0.9909, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 23 val_loss: 0.6617809070487951 val_acc: tensor(0.8277, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 24 train_loss: 0.032344564116626444 train_acc: tensor(0.9902, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 24 val_loss: 0.631158413486532 val_acc: tensor(0.8296, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 25 train_loss: 0.024120670115309103 train_acc: tensor(0.9940, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 25 val_loss: 0.6868527777580167 val_acc: tensor(0.8268, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 26 train_loss: 0.02216946393184896 train_acc: tensor(0.9936, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 26 val_loss: 0.7614834527358377 val_acc: tensor(0.8221, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 27 train_loss: 0.027956713687162845 train_acc: tensor(0.9928, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 27 val_loss: 0.6285566282354351 val_acc: tensor(0.8293, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 28 train_loss: 0.026665695154972906 train_acc: tensor(0.9929, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 28 val_loss: 0.715560358036072 val_acc: tensor(0.8299, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 29 train_loss: 0.025037691841633725 train_acc: tensor(0.9938, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 29 val_loss: 0.7308269022417774 val_acc: tensor(0.8181, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 30 train_loss: 0.014953427917457053 train_acc: tensor(0.9962, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 30 val_loss: 0.7312997441356005 val_acc: tensor(0.8399, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 31 train_loss: 0.014925408801396511 train_acc: tensor(0.9969, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 31 val_loss: 0.7002856274043031 val_acc: tensor(0.8361, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 32 train_loss: 0.027222647509271544 train_acc: tensor(0.9923, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 32 val_loss: 0.7881527107914988 val_acc: tensor(0.8165, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 33 train_loss: 0.02100862292140456 train_acc: tensor(0.9951, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 33 val_loss: 0.6638937954824483 val_acc: tensor(0.8340, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 34 train_loss: 0.02692399190871843 train_acc: tensor(0.9918, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 34 val_loss: 0.7480121110618086 val_acc: tensor(0.8147, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 35 train_loss: 0.02180281992003854 train_acc: tensor(0.9936, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 35 val_loss: 0.6521999860752608 val_acc: tensor(0.8312, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 36 train_loss: 0.0239043119346856 train_acc: tensor(0.9939, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 36 val_loss: 0.7233707066576126 val_acc: tensor(0.8271, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 37 train_loss: 0.018830665778368713 train_acc: tensor(0.9954, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 37 val_loss: 0.7258429318606803 val_acc: tensor(0.8228, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 38 train_loss: 0.019899863106132086 train_acc: tensor(0.9946, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 38 val_loss: 0.6779329382111631 val_acc: tensor(0.8308, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 39 train_loss: 0.014972432617630278 train_acc: tensor(0.9960, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 39 val_loss: 0.7999394011696215 val_acc: tensor(0.8280, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 40 train_loss: 0.018887704972044696 train_acc: tensor(0.9950, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 40 val_loss: 0.8960476247805161 val_acc: tensor(0.7963, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 41 train_loss: 0.019047059232169497 train_acc: tensor(0.9958, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 41 val_loss: 0.7297992579212101 val_acc: tensor(0.8225, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 42 train_loss: 0.023138459394020695 train_acc: tensor(0.9942, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 42 val_loss: 0.8367596670650166 val_acc: tensor(0.7982, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 43 train_loss: 0.021242043529304543 train_acc: tensor(0.9944, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 43 val_loss: 0.7885733230897534 val_acc: tensor(0.8159, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 44 train_loss: 0.015908838384452142 train_acc: tensor(0.9962, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 44 val_loss: 0.6826476035807606 val_acc: tensor(0.8141, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 45 train_loss: 0.01699685258424974 train_acc: tensor(0.9951, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 45 val_loss: 0.8513909940522706 val_acc: tensor(0.8125, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 46 train_loss: 0.018338425389624068 train_acc: tensor(0.9950, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 46 val_loss: 0.6303267884664747 val_acc: tensor(0.8249, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 47 train_loss: 0.016473946693419877 train_acc: tensor(0.9959, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 47 val_loss: 0.7289595421220046 val_acc: tensor(0.8190, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 48 train_loss: 0.0217401935968415 train_acc: tensor(0.9945, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 48 val_loss: 0.8020114913286406 val_acc: tensor(0.8193, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 49 train_loss: 0.01788644833390468 train_acc: tensor(0.9953, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 49 val_loss: 0.6615621052040668 val_acc: tensor(0.8305, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 50 train_loss: 0.018230422891377072 train_acc: tensor(0.9938, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 50 val_loss: 0.6984887218991276 val_acc: tensor(0.8336, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 51 train_loss: 0.018498165097553282 train_acc: tensor(0.9953, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 51 val_loss: 0.7638759007251738 val_acc: tensor(0.8305, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 52 train_loss: 0.01314172713086009 train_acc: tensor(0.9971, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 52 val_loss: 0.7833754950608718 val_acc: tensor(0.8221, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 53 train_loss: 0.017543502645234445 train_acc: tensor(0.9946, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 53 val_loss: 0.7767311691264396 val_acc: tensor(0.8200, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 54 train_loss: 0.0214332040399313 train_acc: tensor(0.9951, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 54 val_loss: 0.8569558745498285 val_acc: tensor(0.7901, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 55 train_loss: 0.01741387154308281 train_acc: tensor(0.9958, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 55 val_loss: 0.5679024869480677 val_acc: tensor(0.8461, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 56 train_loss: 0.017276313546047146 train_acc: tensor(0.9958, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 56 val_loss: 0.8126160011947356 val_acc: tensor(0.7948, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 57 train_loss: 0.015098408972012943 train_acc: tensor(0.9961, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 57 val_loss: 0.6676116990390586 val_acc: tensor(0.8280, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 58 train_loss: 0.013701261379090803 train_acc: tensor(0.9960, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 58 val_loss: 0.724286264586303 val_acc: tensor(0.8141, device='cuda:0', dtype=torch.float64)\n",
      "---------------------------------------------------\n",
      "epoch: 59 train_loss: 0.02656945570438568 train_acc: tensor(0.9930, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 59 val_loss: 0.6199308868473864 val_acc: tensor(0.8262, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#对模型进行迭代训练，对所有的数据训练epoch轮\n",
    "for epoch in range(140):\n",
    "    train_loss_epoch = 0\n",
    "    val_loss_epoch = 0\n",
    "    train_corrects = 0\n",
    "    val_corrects = 0\n",
    "    #对训练数据的加载器进行迭代计算\n",
    "    MyResnet.train().cuda()\n",
    "    for step,(mri,group) in enumerate(train_loader):\n",
    "        ##计算每个batch的损失\n",
    "        output = MyResnet(mri)\n",
    "        loss = loss_func(output,group.long())#交叉熵损失函数\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        optimizer.zero_grad()#每个迭代步的梯度初始化为0\n",
    "        loss.backward()#损失的后向传播，计算梯度\n",
    "        optimizer.step()#使用梯度进行优化\n",
    "        train_loss_epoch += loss.item()*group.size(0)\n",
    "        train_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "    #计算一个epoch的损失和精度\n",
    "    train_loss = train_loss_epoch/len(train_data.group)\n",
    "    train_acc = train_corrects.double()/len(train_data.group)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss,\"train_acc:\",train_acc)\n",
    "     #计算在验证集上的表现\n",
    "    MyResnet.eval()\n",
    "    for step,(mri,group) in enumerate(test_loader):\n",
    "        output = MyResnet(mri)\n",
    "        loss = loss_func(output,group.long())\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        val_loss_epoch += loss.item()*group.size(0)\n",
    "        val_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "\n",
    "    #计算一个epoch上的输出loss和acc\n",
    "    val_loss = val_loss_epoch/len(test_data.group)\n",
    "    val_acc = val_corrects.double()/len(test_data.group)\n",
    "    print(\"epoch:\",epoch,\"val_loss:\",val_loss,\"val_acc:\",val_acc)\n",
    "    #保存每个epoch上的输出loss和acc\n",
    "    historyl.log(epoch,train_loss=train_loss,val_loss = val_loss,train_acc = train_acc.item(),val_acc = val_acc.item())\n",
    "    #可视化网络训练的过程\n",
    "    # with canvasl:\n",
    "    #     canvasl.draw_plot([historyl[\"train_loss\"],historyl[\"val_loss\"]])\n",
    "    #     canvasl.draw_plot([historyl[\"train_acc\"],historyl[\"val_acc\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py372')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e0e02a671462d396ecc20af6161881d6338b1e899f7d69a261eb7f1f955476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
