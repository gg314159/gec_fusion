{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI PET ECA融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" \n",
    "import time\n",
    "# import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pickle as p\n",
    "import hiddenlayer as hl\n",
    "import math\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root,transform_mri= None,transform_pet = None):\n",
    "        super(MyDataset, self).__init__()\n",
    "        MRI_PET_match_all = p.load(open(root,\"rb\"),encoding='iso-8859-1')\n",
    "        MRI = []\n",
    "        PET = []\n",
    "        group = []\n",
    "        for index,row in MRI_PET_match_all.iterrows():\n",
    "            MRI.append(row['MRI_img_array'])\n",
    "            PET.append(row['PET_img_array'])\n",
    "            group.append(row['Group'])\n",
    "        self.MRI = MRI\n",
    "        self.PET = PET\n",
    "        self.group = group  \n",
    "        self.transform_mri = transform_mri\n",
    "        self.transform_pet = transform_pet\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mri = torch.from_numpy(self.MRI[index].transpose(2,0,1)).float()\n",
    "        pet = torch.from_numpy(self.PET[index].transpose(2,0,1)).float()\n",
    "        \n",
    "        \n",
    "        mri = self.transform_mri(mri)\n",
    "        pet = self.transform_pet(pet)\n",
    "      \n",
    "        \n",
    "        group = self.group[index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return mri,pet,group\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.MRI)\n",
    "    \n",
    "    def get_classes_for_all_imgs(self):\n",
    "        return self.group\n",
    "    \n",
    "\n",
    "## transforms模块，进行数据预处理\n",
    "\n",
    "\n",
    "# not16\n",
    "train_mean_mri = [4.1620684, 4.1620684, 4.1620684]\n",
    "train_std_mri = [5.2131376, 5.2131376, 5.2131376]\n",
    "train_mean_pet = [4.081158, 4.081158, 4.081158] \n",
    "train_std_pet = [5.1888165, 5.1888165, 5.1888165]\n",
    "\n",
    "test_mean_mri = [4.1623616, 4.1623616, 4.1623616]\n",
    "test_std_mri = [5.2136188, 5.2136188, 5.2136188]\n",
    "test_mean_pet = [4.106387, 4.106387, 4.106387]\n",
    "test_std_pet = [5.18535, 5.18535, 5.18535]\n",
    "\n",
    "\n",
    "\n",
    "#only one\n",
    "\n",
    "# train_mean_mri = [4.240719, 4.240719, 4.240719]\n",
    "# train_std_mri = [5.254782, 5.254782, 5.254782]\n",
    "# train_mean_pet = [4.106597, 4.106597, 4.106597] \n",
    "# train_std_pet = [5.21822, 5.21822, 5.21822]\n",
    "\n",
    "# test_mean_mri = [4.364004, 4.364004, 4.364004]\n",
    "# test_std_mri = [5.4076633, 5.4076633, 5.4076633] \n",
    "# test_mean_pet = [4.2559123, 4.2559123, 4.2559123]\n",
    "# test_std_pet = [5.373126, 5.373126, 5.373126]\n",
    "\n",
    "# valid_mean_mri = [4.7232037, 4.7232037, 4.7232037]\n",
    "# valid_std_mri =  [5.8529644, 5.8529644, 5.8529644] \n",
    "# valid_mean_pet = [4.5747848, 4.5747848, 4.5747848]\n",
    "# valid_std_pet = [5.809622, 5.809622, 5.809622]\n",
    "\n",
    "#16-in-1\n",
    "# train_mean_mri = [4.176061, 4.176061, 4.176061]\n",
    "# train_std_mri = [5.231413, 5.231413, 5.231413]\n",
    "# train_mean_pet = [4.017313, 4.017313, 4.017313] \n",
    "# train_std_pet = [5.1714053, 5.1714053, 5.1714053]\n",
    "\n",
    "# test_mean_mri = [4.3262687, 4.3262687, 4.3262687]\n",
    "# test_std_mri = [5.419836, 5.419836, 5.419836] \n",
    "# test_mean_pet = [4.1623745, 4.1623745, 4.1623745]\n",
    "# test_std_pet = [5.3586817, 5.3586817, 5.3586817]\n",
    "\n",
    "# valid_mean_mri = [4.7232037, 4.7232037, 4.7232037]\n",
    "# valid_std_mri =  [5.8529644, 5.8529644, 5.8529644] \n",
    "# valid_mean_pet = [4.5747848, 4.5747848, 4.5747848]\n",
    "# valid_std_pet = [5.809622, 5.809622, 5.809622]\n",
    "\n",
    "\n",
    "\n",
    "train_transform_mri = transforms.Compose([\n",
    "    transforms.Normalize(train_mean_mri,train_std_mri),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "])\n",
    "\n",
    "train_transform_pet = transforms.Compose([\n",
    "    transforms.Normalize(train_mean_pet,train_std_pet),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "test_transform_mri = transforms.Compose([\n",
    "    \n",
    "    transforms.Normalize(test_mean_mri,test_std_mri),\n",
    "])\n",
    "\n",
    "test_transform_pet = transforms.Compose([\n",
    "   \n",
    "    transforms.Normalize(test_mean_pet,test_std_pet)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = MyDataset(\"/home/gc/gechang/gec_multi_fusion/end_to_end/train_not16.pkl\", transform_mri =train_transform_mri,transform_pet =train_transform_pet )\n",
    "test_data = MyDataset(\"/home/gc/gechang/gec_multi_fusion/end_to_end/test_not16.pkl\", transform_mri =test_transform_mri,transform_pet =test_transform_pet)\n",
    "\n",
    "\n",
    "\n",
    "# 数据集中，每一类的数目。\n",
    "class_sample_counts = [5568,3297,2338] #not 16\n",
    "#class_sample_counts = [450,237,113] #only one\n",
    "weights = 1./ torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "# 这个 get_classes_for_all_imgs是关键\n",
    "train_targets = train_data.get_classes_for_all_imgs()\n",
    "samples_weights = weights[train_targets]\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 32, num_workers=8, sampler = sampler,shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size = 32, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuECAAttention(nn.Module):\n",
    "    def __init__(self, c, b=1, gamma=2):\n",
    "        super(DuECAAttention,self).__init__()\n",
    "      \n",
    "        t = int(abs((math.log(c, 2) + b) / gamma))\n",
    "        k = t if t % 2 else t + 1 # k只能取奇数\n",
    "\n",
    "        self.avg_pool_ch1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.avg_pool_ch2 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1_ch1 = nn.Conv1d(1, 1, kernel_size=k, padding=int(k/2), bias=False,stride=2) #inchannel = 1,outchannel=1\n",
    "        self.conv1_ch2 = nn.Conv1d(1, 1, kernel_size=k, padding=int(k/2), bias=False,stride=2) #inchannel = 1,outchannel=1\n",
    "        self.sigmoid_ch1 = nn.Sigmoid()\n",
    "        self.sigmoid_ch2 = nn.Sigmoid()\n",
    "        # self.softmax_ch1 = nn.Softmax()\n",
    "        # self.softmax_ch2 = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    #定义网络的前向传播\n",
    "    def forward(self,inp_ch1, inp_ch2):\n",
    "        squeeze_ch1 = self.avg_pool_ch1(inp_ch1) #(n,c,h,w)->(n,c,1,1)\n",
    "        # print(\"squeeze_ch1\",squeeze_ch1.shape)\n",
    "        squeeze_ch2 = self.avg_pool_ch2(inp_ch2)\n",
    "        # print(\"squeeze_ch2\",squeeze_ch2.shape)\n",
    "        squeeze_comb = torch.cat((squeeze_ch1, squeeze_ch2), 1)  # [B, C*2]\n",
    "        # print(\"squeeze_comb\",squeeze_comb.shape)\n",
    "        fc_ch1 = self.conv1_ch1(squeeze_comb.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        fc_ch2 = self.conv1_ch2(squeeze_comb.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        # print(\"fc_comb\",fc_comb.shape)\n",
    "        out_ch1 = self.sigmoid_ch1(fc_ch1)\n",
    "        out_ch2 = self.sigmoid_ch1(fc_ch2)\n",
    "        # out_ch1 = self.softmax_ch1(fc_ch1)\n",
    "        # out_ch2 = self.softmax_ch2(fc_ch2)\n",
    "        return out_ch1,out_ch2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuBasicBlock(nn.Module):      # 左侧的 residual block 结构（18-layer、34-layer）\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):      # 两层卷积 Conv2d + Shutcuts\n",
    "        super(DuBasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1_ch1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.conv1_ch2 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1_ch1 = nn.BatchNorm2d(planes)\n",
    "        self.bn1_ch2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv2_ch1 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.conv2_ch2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.bn2_ch1 = nn.BatchNorm2d(planes)\n",
    "        self.bn2_ch2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.channel = DuECAAttention(planes)       # Efficient Channel Attention module\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:      # Shutcuts用于构建 Conv Block 和 Identity Block\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "            \n",
    "\n",
    "    def forward(self, c):\n",
    "        c1,c2 = c.chunk(2,dim=1)\n",
    "        # print(\"c1shape:\",c1.shape)\n",
    "        # print(\"c2shape:\",c2.shape)\n",
    "        \n",
    "        out1 = F.relu(self.bn1_ch1(self.conv1_ch1(c1)))\n",
    "        out2 = F.relu(self.bn1_ch2(self.conv1_ch2(c2)))\n",
    "        \n",
    "        out1 = self.bn2_ch1(self.conv2_ch1(out1))\n",
    "        out2 = self.bn2_ch2(self.conv2_ch2(out2))\n",
    "        \n",
    "        ECA_out1,ECA_out2 = self.channel(out1,out2)\n",
    "        \n",
    "        out1 = out1 * ECA_out1\n",
    "        out2 = out2 * ECA_out2\n",
    "        \n",
    "        out1 += self.shortcut(c1)\n",
    "        out1 += self.shortcut(c2)\n",
    "        \n",
    "        out1 = F.relu(out1)\n",
    "        out2 = F.relu(out2)\n",
    "        \n",
    "        out = torch.cat((out1,out2),dim =1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA_ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=3): #block有两种，BasicBlock或者Bottleneck，每个block里都有一个通道注意力\n",
    "        super(ECA_ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1_ch1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)                  # conv1\n",
    "        self.conv1_ch2 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)  \n",
    "        \n",
    "        self.bn1_ch1 = nn.BatchNorm2d(64)\n",
    "        self.bn1_ch2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)       # conv2_x\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)      # conv3_x\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)      # conv4_x\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)      # conv5_x\n",
    "        \n",
    "        self.avgpool_ch1 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.avgpool_ch2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # self.linear_ch1 = nn.Linear(512 * block.expansion, num_classes)\n",
    "        # self.linear_ch2 = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        self.linear =  nn.Linear(1024 * block.expansion, 3)\n",
    "        \n",
    "        # self.fc = nn.Linear(6, 3, bias=True)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, c1,c2):\n",
    "        c1 = F.relu(self.bn1_ch1(self.conv1_ch1(c1)))\n",
    "        c2 = F.relu(self.bn1_ch2(self.conv1_ch2(c2))) \n",
    "        \n",
    "        # nn.Sequential()定义的网络，只能接受单输入，同时输出也只能是单输出。  \n",
    "        # 解决方法，可以将两个tensor经过torch.cat组合在一起，然后在forward()中将组合拆分\n",
    "        \n",
    "        c = torch.cat((c1,c2),dim =1)\n",
    "        \n",
    "        c = self.layer1(c)\n",
    "        c = self.layer2(c)\n",
    "        c = self.layer3(c)\n",
    "        c = self.layer4(c)\n",
    "        \n",
    "        c1,c2 = c.chunk(2,dim=1)\n",
    "        \n",
    "        c1 = self.avgpool_ch1(c1)\n",
    "        c2 = self.avgpool_ch2(c2)\n",
    "        \n",
    "        c1 = torch.flatten(c1, 1)\n",
    "        c2 = torch.flatten(c2, 1)\n",
    "        \n",
    "        # c1 = self.linear_ch1(c1)\n",
    "        # c2 = self.linear_ch2(c2)\n",
    "        \n",
    "        comb = torch.cat((c1, c2), 1)\n",
    "        out = self.linear(comb)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECA_ResNet18():\n",
    "    return ECA_ResNet(DuBasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECAfusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECAfusionModel,self).__init__()\n",
    "      \n",
    "        self.ECA = ECA_ResNet18()\n",
    "        \n",
    "    #定义网络的前向传播\n",
    "    def forward(self,MRI,PET):\n",
    "        output = self.ECA(MRI,PET)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: 0.9228316228730338 train_acc: tensor(0.5435, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 0 val_loss: 1.1890404281479803 val_acc: tensor(0.3576, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 1 train_loss: 0.521195199745042 train_acc: tensor(0.7956, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 1 val_loss: 1.4612564016504221 val_acc: tensor(0.3980, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 2 train_loss: 0.2959093302914074 train_acc: tensor(0.9107, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 2 val_loss: 1.3194523151982482 val_acc: tensor(0.6340, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 3 train_loss: 0.18141642821686607 train_acc: tensor(0.9527, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 3 val_loss: 0.6745115713545339 val_acc: tensor(0.7158, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 4 train_loss: 0.1208037163636514 train_acc: tensor(0.9754, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 4 val_loss: 1.8930285417368238 val_acc: tensor(0.6054, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 5 train_loss: 0.08461718380451203 train_acc: tensor(0.9834, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 5 val_loss: 0.6610347305734953 val_acc: tensor(0.7245, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 6 train_loss: 0.06742115394345352 train_acc: tensor(0.9876, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 6 val_loss: 0.5944419537684811 val_acc: tensor(0.7767, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 7 train_loss: 0.04824814415536821 train_acc: tensor(0.9919, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 7 val_loss: 0.8129290301371274 val_acc: tensor(0.7093, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 8 train_loss: 0.04218274659078036 train_acc: tensor(0.9923, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 8 val_loss: 0.7045568203674027 val_acc: tensor(0.7394, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 9 train_loss: 0.03041273376904428 train_acc: tensor(0.9966, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 9 val_loss: 1.3679892965007472 val_acc: tensor(0.6418, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 10 train_loss: 0.03187645768347595 train_acc: tensor(0.9945, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 10 val_loss: 1.0676098153393585 val_acc: tensor(0.6934, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 11 train_loss: 0.029166109040379525 train_acc: tensor(0.9959, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 11 val_loss: 0.7138976815587549 val_acc: tensor(0.7565, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 12 train_loss: 0.02443335357122123 train_acc: tensor(0.9954, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 12 val_loss: 2.927758436986189 val_acc: tensor(0.5731, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 13 train_loss: 0.02583363024850509 train_acc: tensor(0.9952, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 13 val_loss: 0.6659058318728238 val_acc: tensor(0.7708, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 14 train_loss: 0.030962105855079632 train_acc: tensor(0.9932, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 14 val_loss: 3.247223851168893 val_acc: tensor(0.2674, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 15 train_loss: 0.01897581940483568 train_acc: tensor(0.9968, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 15 val_loss: 0.7020594277210644 val_acc: tensor(0.7643, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 16 train_loss: 0.024284406695847534 train_acc: tensor(0.9944, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 16 val_loss: 1.7189645789655166 val_acc: tensor(0.5731, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 17 train_loss: 0.02454131354611101 train_acc: tensor(0.9943, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 17 val_loss: 0.8017419382764156 val_acc: tensor(0.7528, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105650/745567622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#每个迭代步的梯度初始化为0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#损失的后向传播，计算梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#使用梯度进行优化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtrain_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_lab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py372/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py372/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py372/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;31m# update the steps for each param group update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                     \u001b[0;31m# record the step after step update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ECAfusionModel()\n",
    "model = model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "#记录训练过程指标\n",
    "historyl = hl.History()\n",
    "#使用Canves进行可视化\n",
    "canvasl = hl.Canvas()\n",
    "\n",
    "min_loss = 2\n",
    "\n",
    "#对模型进行迭代训练，对所有的数据训练epoch轮\n",
    "for epoch in range(100):\n",
    "    train_loss_epoch = 0\n",
    "    val_loss_epoch = 0\n",
    "    train_corrects = 0\n",
    "    val_corrects = 0\n",
    "    #对训练数据的加载器进行迭代计算\n",
    "    model.train().cuda()\n",
    "    for step,(mri,pet,group) in enumerate(train_loader):\n",
    "        ##计算每个batch的损失\n",
    "        mri = mri.to(DEVICE)\n",
    "        pet = pet.to(DEVICE)\n",
    "        group = group.to(DEVICE)\n",
    "        output = model(mri,pet)\n",
    "        loss = criterion(output,group)#交叉熵损失函数\n",
    "        pre_lab = torch.argmax(output,1)\n",
    "        optimizer.zero_grad()#每个迭代步的梯度初始化为0\n",
    "        loss.backward()#损失的后向传播，计算梯度\n",
    "        optimizer.step()#使用梯度进行优化\n",
    "        train_loss_epoch += loss.item()*group.size(0)\n",
    "        train_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "    #计算一个epoch的损失和精度\n",
    "    train_loss = train_loss_epoch/len(train_data.group)\n",
    "    train_acc = train_corrects.double()/len(train_data.group)\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss,\"train_acc:\",train_acc)\n",
    "    \n",
    "    #计算在验证集上的表现\n",
    "    model.eval()\n",
    "    for step,(mri,pet,group) in enumerate(test_loader):\n",
    "        mri = mri.to(DEVICE)\n",
    "        pet = pet.to(DEVICE)\n",
    "        group = group.to(DEVICE)\n",
    "        output = model(mri,pet)\n",
    "        loss = criterion(output,group.to(DEVICE))\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        val_loss_epoch += loss.item()*group.size(0)\n",
    "        val_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "\n",
    "    #计算一个epoch上的输出loss和acc\n",
    "    val_loss = val_loss_epoch/len(test_data.group)\n",
    "    val_acc = val_corrects.double()/len(test_data.group)\n",
    "    print(\"epoch:\",epoch,\"val_loss:\",val_loss,\"val_acc:\",val_acc)\n",
    "    \n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        print(\"save model\")\n",
    "        # 保存模型语句\n",
    "        torch.save(model.state_dict(),\"model_\"+str(val_acc)+\".pth\")\n",
    "    # #保存每个epoch上的输出loss和acc\n",
    "    historyl.log(epoch,train_loss=train_loss,val_loss = val_loss,train_acc = train_acc.item(),val_acc = val_acc.item())\n",
    "    # #可视化网络训练的过程\n",
    "    # with canvasl:\n",
    "    #     canvasl.draw_plot([historyl[\"train_loss\"],historyl[\"val_loss\"]])\n",
    "    #     canvasl.draw_plot([historyl[\"train_acc\"],historyl[\"val_acc\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载模型\n",
    "\n",
    "\n",
    "model = ECAfusionModel().cuda()\n",
    " \n",
    "model.load_state_dict(torch.load('/home/gc/gechang/gec_multi_fusion/pytorch/model_tensor(0.7767, device=\\'cuda:0\\', dtype=torch.float64).pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECAfusionModel(\n",
       "  (ECA): ECA_ResNet(\n",
       "    (conv1_ch1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv1_ch2): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1_ch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn1_ch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DuBasicBlock(\n",
       "        (conv1_ch1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv1_ch2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1_ch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn1_ch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2_ch1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2_ch2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2_ch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2_ch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (channel): DuECAAttention(\n",
       "          (avg_pool_ch1): AdaptiveAvgPool2d(output_size=1)\n",
       "          (avg_pool_ch2): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv1_ch1): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (conv1_ch2): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
       "          (sigmoid_ch1): Sigmoid()\n",
       "          (sigmoid_ch2): Sigmoid()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (avgpool_ch1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (avgpool_ch2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (linear): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRI_PET= p.load(open('/home/gc/gechang/gec_multi_fusion/end_to_end/train_not16.pkl',\"rb\"),encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "5024\n",
      "5025\n",
      "5026\n",
      "5027\n",
      "5028\n",
      "5029\n",
      "5030\n",
      "5031\n",
      "5032\n",
      "5033\n",
      "5034\n",
      "5035\n",
      "5036\n",
      "5037\n",
      "5038\n",
      "5039\n",
      "5072\n",
      "5073\n",
      "5074\n",
      "5075\n",
      "5076\n",
      "5077\n",
      "5078\n",
      "5079\n",
      "5080\n",
      "5081\n",
      "5082\n",
      "5083\n",
      "5084\n",
      "5085\n",
      "5086\n",
      "5087\n",
      "5216\n",
      "5217\n",
      "5218\n",
      "5219\n",
      "5220\n",
      "5221\n",
      "5222\n",
      "5223\n",
      "5224\n",
      "5225\n",
      "5226\n",
      "5227\n",
      "5228\n",
      "5229\n",
      "5230\n",
      "5231\n",
      "5408\n",
      "5409\n",
      "5410\n",
      "5411\n",
      "5412\n",
      "5413\n",
      "5414\n",
      "5415\n",
      "5416\n",
      "5417\n",
      "5418\n",
      "5419\n",
      "5420\n",
      "5421\n",
      "5422\n",
      "5423\n",
      "5776\n",
      "5777\n",
      "5778\n",
      "5779\n",
      "5780\n",
      "5781\n",
      "5782\n",
      "5783\n",
      "5784\n",
      "5785\n",
      "5786\n",
      "5787\n",
      "5788\n",
      "5789\n",
      "5790\n",
      "5791\n",
      "5792\n",
      "5793\n",
      "5794\n",
      "5795\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5799\n",
      "5800\n",
      "5801\n",
      "5802\n",
      "5803\n",
      "5804\n",
      "5805\n",
      "5806\n",
      "5807\n",
      "5808\n",
      "5809\n",
      "5810\n",
      "5811\n",
      "5812\n",
      "5813\n",
      "5814\n",
      "5815\n",
      "5816\n",
      "5817\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5821\n",
      "5822\n",
      "5823\n",
      "5856\n",
      "5857\n",
      "5858\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5862\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5868\n",
      "5869\n",
      "5870\n",
      "5871\n",
      "5872\n",
      "5873\n",
      "5874\n",
      "5875\n",
      "5876\n",
      "5877\n",
      "5878\n",
      "5879\n",
      "5880\n",
      "5881\n",
      "5882\n",
      "5883\n",
      "5884\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5890\n",
      "5891\n",
      "5892\n",
      "5893\n",
      "5894\n",
      "5895\n",
      "5896\n",
      "5897\n",
      "5898\n",
      "5899\n",
      "5900\n",
      "5901\n",
      "5902\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5922\n",
      "5923\n",
      "5924\n",
      "5925\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5931\n",
      "5932\n",
      "5933\n",
      "5934\n",
      "5935\n",
      "5952\n",
      "5953\n",
      "5954\n",
      "5955\n",
      "5956\n",
      "5957\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5961\n",
      "5962\n",
      "5963\n",
      "5964\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5969\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5978\n",
      "5979\n",
      "5980\n",
      "5981\n",
      "5982\n",
      "5983\n",
      "5984\n",
      "5985\n",
      "5986\n",
      "5987\n",
      "5988\n",
      "5989\n",
      "5990\n",
      "5991\n",
      "5992\n",
      "5993\n",
      "5994\n",
      "5995\n",
      "5996\n",
      "5997\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6006\n",
      "6007\n",
      "6008\n",
      "6009\n",
      "6010\n",
      "6011\n",
      "6012\n",
      "6013\n",
      "6014\n",
      "6015\n",
      "6016\n",
      "6017\n",
      "6018\n",
      "6019\n",
      "6020\n",
      "6021\n",
      "6022\n",
      "6023\n",
      "6024\n",
      "6025\n",
      "6026\n",
      "6027\n",
      "6028\n",
      "6029\n",
      "6030\n",
      "6031\n",
      "6048\n",
      "6049\n",
      "6050\n",
      "6051\n",
      "6052\n",
      "6053\n",
      "6054\n",
      "6055\n",
      "6056\n",
      "6057\n",
      "6058\n",
      "6059\n",
      "6060\n",
      "6061\n",
      "6062\n",
      "6063\n",
      "6496\n",
      "6497\n",
      "6498\n",
      "6499\n",
      "6500\n",
      "6501\n",
      "6502\n",
      "6503\n",
      "6504\n",
      "6505\n",
      "6506\n",
      "6507\n",
      "6508\n",
      "6509\n",
      "6510\n",
      "6511\n",
      "7088\n",
      "7089\n",
      "7090\n",
      "7091\n",
      "7092\n",
      "7093\n",
      "7094\n",
      "7095\n",
      "7096\n",
      "7097\n",
      "7098\n",
      "7099\n",
      "7100\n",
      "7101\n",
      "7102\n",
      "7103\n",
      "7104\n",
      "7105\n",
      "7106\n",
      "7107\n",
      "7108\n",
      "7109\n",
      "7110\n",
      "7111\n",
      "7112\n",
      "7113\n",
      "7114\n",
      "7115\n",
      "7116\n",
      "7117\n",
      "7118\n",
      "7119\n",
      "7120\n",
      "7121\n",
      "7122\n",
      "7123\n",
      "7124\n",
      "7125\n",
      "7126\n",
      "7127\n",
      "7128\n",
      "7129\n",
      "7130\n",
      "7131\n",
      "7132\n",
      "7133\n",
      "7134\n",
      "7135\n",
      "7728\n",
      "7729\n",
      "7730\n",
      "7731\n",
      "7732\n",
      "7733\n",
      "7734\n",
      "7735\n",
      "7736\n",
      "7737\n",
      "7738\n",
      "7739\n",
      "7740\n",
      "7741\n",
      "7742\n",
      "7743\n",
      "8224\n",
      "8225\n",
      "8226\n",
      "8227\n",
      "8228\n",
      "8229\n",
      "8230\n",
      "8231\n",
      "8232\n",
      "8233\n",
      "8234\n",
      "8235\n",
      "8236\n",
      "8237\n",
      "8238\n",
      "8239\n",
      "8400\n",
      "8401\n",
      "8402\n",
      "8403\n",
      "8404\n",
      "8405\n",
      "8406\n",
      "8407\n",
      "8408\n",
      "8409\n",
      "8410\n",
      "8411\n",
      "8412\n",
      "8413\n",
      "8414\n",
      "8415\n",
      "8416\n",
      "8417\n",
      "8418\n",
      "8419\n",
      "8420\n",
      "8421\n",
      "8422\n",
      "8423\n",
      "8424\n",
      "8425\n",
      "8426\n",
      "8427\n",
      "8428\n",
      "8429\n",
      "8430\n",
      "8431\n",
      "8432\n",
      "8433\n",
      "8434\n",
      "8435\n",
      "8436\n",
      "8437\n",
      "8438\n",
      "8439\n",
      "8440\n",
      "8441\n",
      "8442\n",
      "8443\n",
      "8444\n",
      "8445\n",
      "8446\n",
      "8447\n",
      "8688\n",
      "8689\n",
      "8690\n",
      "8691\n",
      "8692\n",
      "8693\n",
      "8694\n",
      "8695\n",
      "8696\n",
      "8697\n",
      "8698\n",
      "8699\n",
      "8700\n",
      "8701\n",
      "8702\n",
      "8703\n",
      "8864\n",
      "8865\n",
      "8866\n",
      "8867\n",
      "8868\n",
      "8869\n",
      "8870\n",
      "8871\n",
      "8872\n",
      "8873\n",
      "8874\n",
      "8875\n",
      "8876\n",
      "8877\n",
      "8878\n",
      "8879\n",
      "8896\n",
      "8897\n",
      "8898\n",
      "8899\n",
      "8900\n",
      "8901\n",
      "8902\n",
      "8903\n",
      "8904\n",
      "8905\n",
      "8906\n",
      "8907\n",
      "8908\n",
      "8909\n",
      "8910\n",
      "8911\n",
      "9136\n",
      "9137\n",
      "9138\n",
      "9139\n",
      "9140\n",
      "9141\n",
      "9142\n",
      "9143\n",
      "9144\n",
      "9145\n",
      "9146\n",
      "9147\n",
      "9148\n",
      "9149\n",
      "9150\n",
      "9151\n",
      "9232\n",
      "9233\n",
      "9234\n",
      "9235\n",
      "9236\n",
      "9237\n",
      "9238\n",
      "9239\n",
      "9240\n",
      "9241\n",
      "9242\n",
      "9243\n",
      "9244\n",
      "9245\n",
      "9246\n",
      "9247\n",
      "9248\n",
      "9249\n",
      "9250\n",
      "9251\n",
      "9252\n",
      "9253\n",
      "9254\n",
      "9255\n",
      "9256\n",
      "9257\n",
      "9258\n",
      "9259\n",
      "9260\n",
      "9261\n",
      "9262\n",
      "9263\n",
      "9360\n",
      "9361\n",
      "9362\n",
      "9363\n",
      "9364\n",
      "9365\n",
      "9366\n",
      "9367\n",
      "9368\n",
      "9369\n",
      "9370\n",
      "9371\n",
      "9372\n",
      "9373\n",
      "9374\n",
      "9375\n",
      "9376\n",
      "9377\n",
      "9378\n",
      "9379\n",
      "9380\n",
      "9381\n",
      "9382\n",
      "9383\n",
      "9384\n",
      "9385\n",
      "9386\n",
      "9387\n",
      "9388\n",
      "9389\n",
      "9390\n",
      "9391\n",
      "9392\n",
      "9393\n",
      "9394\n",
      "9395\n",
      "9396\n",
      "9397\n",
      "9398\n",
      "9399\n",
      "9400\n",
      "9401\n",
      "9402\n",
      "9403\n",
      "9404\n",
      "9405\n",
      "9406\n",
      "9407\n",
      "9760\n",
      "9761\n",
      "9762\n",
      "9763\n",
      "9764\n",
      "9765\n",
      "9766\n",
      "9767\n",
      "9768\n",
      "9769\n",
      "9770\n",
      "9771\n",
      "9772\n",
      "9773\n",
      "9774\n",
      "9775\n",
      "9856\n",
      "9857\n",
      "9858\n",
      "9859\n",
      "9860\n",
      "9861\n",
      "9862\n",
      "9863\n",
      "9864\n",
      "9865\n",
      "9866\n",
      "9867\n",
      "9868\n",
      "9869\n",
      "9870\n",
      "9871\n",
      "9872\n",
      "9873\n",
      "9874\n",
      "9875\n",
      "9876\n",
      "9877\n",
      "9878\n",
      "9879\n",
      "9880\n",
      "9881\n",
      "9882\n",
      "9883\n",
      "9884\n",
      "9885\n",
      "9886\n",
      "9887\n",
      "9888\n",
      "9889\n",
      "9890\n",
      "9891\n",
      "9892\n",
      "9893\n",
      "9894\n",
      "9895\n",
      "9896\n",
      "9897\n",
      "9898\n",
      "9899\n",
      "9900\n",
      "9901\n",
      "9902\n",
      "9903\n",
      "9968\n",
      "9969\n",
      "9970\n",
      "9971\n",
      "9972\n",
      "9973\n",
      "9974\n",
      "9975\n",
      "9976\n",
      "9977\n",
      "9978\n",
      "9979\n",
      "9980\n",
      "9981\n",
      "9982\n",
      "9983\n",
      "9984\n",
      "9985\n",
      "9986\n",
      "9987\n",
      "9988\n",
      "9989\n",
      "9990\n",
      "9991\n",
      "9992\n",
      "9993\n",
      "9994\n",
      "9995\n",
      "9996\n",
      "9997\n",
      "9998\n",
      "9999\n",
      "10000\n",
      "10001\n",
      "10002\n",
      "10003\n",
      "10004\n",
      "10005\n",
      "10006\n",
      "10007\n",
      "10008\n",
      "10009\n",
      "10010\n",
      "10011\n",
      "10012\n",
      "10013\n",
      "10014\n",
      "10015\n",
      "10112\n",
      "10113\n",
      "10114\n",
      "10115\n",
      "10116\n",
      "10117\n",
      "10118\n",
      "10119\n",
      "10120\n",
      "10121\n",
      "10122\n",
      "10123\n",
      "10124\n",
      "10125\n",
      "10126\n",
      "10127\n",
      "10128\n",
      "10129\n",
      "10130\n",
      "10131\n",
      "10132\n",
      "10133\n",
      "10134\n",
      "10135\n",
      "10136\n",
      "10137\n",
      "10138\n",
      "10139\n",
      "10140\n",
      "10141\n",
      "10142\n",
      "10143\n",
      "10448\n",
      "10449\n",
      "10450\n",
      "10451\n",
      "10452\n",
      "10453\n",
      "10454\n",
      "10455\n",
      "10456\n",
      "10457\n",
      "10458\n",
      "10459\n",
      "10460\n",
      "10461\n",
      "10462\n",
      "10463\n",
      "10464\n",
      "10465\n",
      "10466\n",
      "10467\n",
      "10468\n",
      "10469\n",
      "10470\n",
      "10471\n",
      "10472\n",
      "10473\n",
      "10474\n",
      "10475\n",
      "10476\n",
      "10477\n",
      "10478\n",
      "10479\n",
      "10480\n",
      "10481\n",
      "10482\n",
      "10483\n",
      "10484\n",
      "10485\n",
      "10486\n",
      "10487\n",
      "10488\n",
      "10489\n",
      "10490\n",
      "10491\n",
      "10492\n",
      "10493\n",
      "10494\n",
      "10495\n",
      "10656\n",
      "10657\n",
      "10658\n",
      "10659\n",
      "10660\n",
      "10661\n",
      "10662\n",
      "10663\n",
      "10664\n",
      "10665\n",
      "10666\n",
      "10667\n",
      "10668\n",
      "10669\n",
      "10670\n",
      "10671\n",
      "10672\n",
      "10673\n",
      "10674\n",
      "10675\n",
      "10676\n",
      "10677\n",
      "10678\n",
      "10679\n",
      "10680\n",
      "10681\n",
      "10682\n",
      "10683\n",
      "10684\n",
      "10685\n",
      "10686\n",
      "10687\n",
      "10688\n",
      "10689\n",
      "10690\n",
      "10691\n",
      "10692\n",
      "10693\n",
      "10694\n",
      "10695\n",
      "10696\n",
      "10697\n",
      "10698\n",
      "10699\n",
      "10700\n",
      "10701\n",
      "10702\n",
      "10703\n",
      "10704\n",
      "10705\n",
      "10706\n",
      "10707\n",
      "10708\n",
      "10709\n",
      "10710\n",
      "10711\n",
      "10712\n",
      "10713\n",
      "10714\n",
      "10715\n",
      "10716\n",
      "10717\n",
      "10718\n",
      "10719\n",
      "10720\n",
      "10721\n",
      "10722\n",
      "10723\n",
      "10724\n",
      "10725\n",
      "10726\n",
      "10727\n",
      "10728\n",
      "10729\n",
      "10730\n",
      "10731\n",
      "10732\n",
      "10733\n",
      "10734\n",
      "10735\n",
      "10736\n",
      "10737\n",
      "10738\n",
      "10739\n",
      "10740\n",
      "10741\n",
      "10742\n",
      "10743\n",
      "10744\n",
      "10745\n",
      "10746\n",
      "10747\n",
      "10748\n",
      "10749\n",
      "10750\n",
      "10751\n",
      "10752\n",
      "10753\n",
      "10754\n",
      "10755\n",
      "10756\n",
      "10757\n",
      "10758\n",
      "10759\n",
      "10760\n",
      "10761\n",
      "10762\n",
      "10763\n",
      "10764\n",
      "10765\n",
      "10766\n",
      "10767\n",
      "11104\n",
      "11105\n",
      "11106\n",
      "11107\n",
      "11108\n",
      "11109\n",
      "11110\n",
      "11111\n",
      "11112\n",
      "11113\n",
      "11114\n",
      "11115\n",
      "11116\n",
      "11117\n",
      "11118\n",
      "11119\n",
      "11136\n",
      "11137\n",
      "11138\n",
      "11139\n",
      "11140\n",
      "11141\n",
      "11142\n",
      "11143\n",
      "11144\n",
      "11145\n",
      "11146\n",
      "11147\n",
      "11148\n",
      "11149\n",
      "11150\n",
      "11151\n",
      "11152\n",
      "11153\n",
      "11154\n",
      "11155\n",
      "11156\n",
      "11157\n",
      "11158\n",
      "11159\n",
      "11160\n",
      "11161\n",
      "11162\n",
      "11163\n",
      "11164\n",
      "11165\n",
      "11166\n",
      "11167\n",
      "11168\n",
      "11169\n",
      "11170\n",
      "11171\n",
      "11172\n",
      "11173\n",
      "11174\n",
      "11175\n",
      "11176\n",
      "11177\n",
      "11178\n",
      "11179\n",
      "11180\n",
      "11181\n",
      "11182\n",
      "11183\n",
      "11184\n",
      "11185\n",
      "11186\n",
      "11187\n",
      "11188\n",
      "11189\n",
      "11190\n",
      "11191\n",
      "11192\n",
      "11193\n",
      "11194\n",
      "11195\n",
      "11196\n",
      "11197\n",
      "11198\n",
      "11199\n",
      "11536\n",
      "11537\n",
      "11538\n",
      "11539\n",
      "11540\n",
      "11541\n",
      "11542\n",
      "11543\n",
      "11544\n",
      "11545\n",
      "11546\n",
      "11547\n",
      "11548\n",
      "11549\n",
      "11550\n",
      "11551\n",
      "11552\n",
      "11553\n",
      "11554\n",
      "11555\n",
      "11556\n",
      "11557\n",
      "11558\n",
      "11559\n",
      "11560\n",
      "11561\n",
      "11562\n",
      "11563\n",
      "11564\n",
      "11565\n",
      "11566\n",
      "11567\n",
      "11808\n",
      "11809\n",
      "11810\n",
      "11811\n",
      "11812\n",
      "11813\n",
      "11814\n",
      "11815\n",
      "11816\n",
      "11817\n",
      "11818\n",
      "11819\n",
      "11820\n",
      "11821\n",
      "11822\n",
      "11823\n",
      "11888\n",
      "11889\n",
      "11890\n",
      "11891\n",
      "11892\n",
      "11893\n",
      "11894\n",
      "11895\n",
      "11896\n",
      "11897\n",
      "11898\n",
      "11899\n",
      "11900\n",
      "11901\n",
      "11902\n",
      "11903\n",
      "12032\n",
      "12033\n",
      "12034\n",
      "12035\n",
      "12036\n",
      "12037\n",
      "12038\n",
      "12039\n",
      "12040\n",
      "12041\n",
      "12042\n",
      "12043\n",
      "12044\n",
      "12045\n",
      "12046\n",
      "12047\n",
      "12048\n",
      "12049\n",
      "12050\n",
      "12051\n",
      "12052\n",
      "12053\n",
      "12054\n",
      "12055\n",
      "12056\n",
      "12057\n",
      "12058\n",
      "12059\n",
      "12060\n",
      "12061\n",
      "12062\n",
      "12063\n",
      "12064\n",
      "12065\n",
      "12066\n",
      "12067\n",
      "12068\n",
      "12069\n",
      "12070\n",
      "12071\n",
      "12072\n",
      "12073\n",
      "12074\n",
      "12075\n",
      "12076\n",
      "12077\n",
      "12078\n",
      "12079\n",
      "12320\n",
      "12321\n",
      "12322\n",
      "12323\n",
      "12324\n",
      "12325\n",
      "12326\n",
      "12327\n",
      "12328\n",
      "12329\n",
      "12330\n",
      "12331\n",
      "12332\n",
      "12333\n",
      "12334\n",
      "12335\n",
      "12336\n",
      "12337\n",
      "12338\n",
      "12339\n",
      "12340\n",
      "12341\n",
      "12342\n",
      "12343\n",
      "12344\n",
      "12345\n",
      "12346\n",
      "12347\n",
      "12348\n",
      "12349\n",
      "12350\n",
      "12351\n",
      "12352\n",
      "12353\n",
      "12354\n",
      "12355\n",
      "12356\n",
      "12357\n",
      "12358\n",
      "12359\n",
      "12360\n",
      "12361\n",
      "12362\n",
      "12363\n",
      "12364\n",
      "12365\n",
      "12366\n",
      "12367\n",
      "12704\n",
      "12705\n",
      "12706\n",
      "12707\n",
      "12708\n",
      "12709\n",
      "12710\n",
      "12711\n",
      "12712\n",
      "12713\n",
      "12714\n",
      "12715\n",
      "12716\n",
      "12717\n",
      "12718\n",
      "12719\n",
      "12720\n",
      "12721\n",
      "12722\n",
      "12723\n",
      "12724\n",
      "12725\n",
      "12726\n",
      "12727\n",
      "12728\n",
      "12729\n",
      "12730\n",
      "12731\n",
      "12732\n",
      "12733\n",
      "12734\n",
      "12735\n",
      "12880\n",
      "12881\n",
      "12882\n",
      "12883\n",
      "12884\n",
      "12885\n",
      "12886\n",
      "12887\n",
      "12888\n",
      "12889\n",
      "12890\n",
      "12891\n",
      "12892\n",
      "12893\n",
      "12894\n",
      "12895\n",
      "12912\n",
      "12913\n",
      "12914\n",
      "12915\n",
      "12916\n",
      "12917\n",
      "12918\n",
      "12919\n",
      "12920\n",
      "12921\n",
      "12922\n",
      "12923\n",
      "12924\n",
      "12925\n",
      "12926\n",
      "12927\n",
      "12976\n",
      "12977\n",
      "12978\n",
      "12979\n",
      "12980\n",
      "12981\n",
      "12982\n",
      "12983\n",
      "12984\n",
      "12985\n",
      "12986\n",
      "12987\n",
      "12988\n",
      "12989\n",
      "12990\n",
      "12991\n",
      "13056\n",
      "13057\n",
      "13058\n",
      "13059\n",
      "13060\n",
      "13061\n",
      "13062\n",
      "13063\n",
      "13064\n",
      "13065\n",
      "13066\n",
      "13067\n",
      "13068\n",
      "13069\n",
      "13070\n",
      "13071\n",
      "13440\n",
      "13441\n",
      "13442\n",
      "13443\n",
      "13444\n",
      "13445\n",
      "13446\n",
      "13447\n",
      "13448\n",
      "13449\n",
      "13450\n",
      "13451\n",
      "13452\n",
      "13453\n",
      "13454\n",
      "13455\n",
      "13456\n",
      "13457\n",
      "13458\n",
      "13459\n",
      "13460\n",
      "13461\n",
      "13462\n",
      "13463\n",
      "13464\n",
      "13465\n",
      "13466\n",
      "13467\n",
      "13468\n",
      "13469\n",
      "13470\n",
      "13471\n",
      "13888\n",
      "13889\n",
      "13890\n",
      "13891\n",
      "13892\n",
      "13893\n",
      "13894\n",
      "13895\n",
      "13896\n",
      "13897\n",
      "13898\n",
      "13899\n",
      "13900\n",
      "13901\n",
      "13902\n",
      "13903\n",
      "13904\n",
      "13905\n",
      "13906\n",
      "13907\n",
      "13908\n",
      "13909\n",
      "13910\n",
      "13911\n",
      "13912\n",
      "13913\n",
      "13914\n",
      "13915\n",
      "13916\n",
      "13917\n",
      "13918\n",
      "13919\n",
      "13920\n",
      "13921\n",
      "13922\n",
      "13923\n",
      "13924\n",
      "13925\n",
      "13926\n",
      "13927\n",
      "13928\n",
      "13929\n",
      "13930\n",
      "13931\n",
      "13932\n",
      "13933\n",
      "13934\n",
      "13935\n",
      "13936\n",
      "13937\n",
      "13938\n",
      "13939\n",
      "13940\n",
      "13941\n",
      "13942\n",
      "13943\n",
      "13944\n",
      "13945\n",
      "13946\n",
      "13947\n",
      "13948\n",
      "13949\n",
      "13950\n",
      "13951\n",
      "14176\n",
      "14177\n",
      "14178\n",
      "14179\n",
      "14180\n",
      "14181\n",
      "14182\n",
      "14183\n",
      "14184\n",
      "14185\n",
      "14186\n",
      "14187\n",
      "14188\n",
      "14189\n",
      "14190\n",
      "14191\n",
      "14288\n",
      "14289\n",
      "14290\n",
      "14291\n",
      "14292\n",
      "14293\n",
      "14294\n",
      "14295\n",
      "14296\n",
      "14297\n",
      "14298\n",
      "14299\n",
      "14300\n",
      "14301\n",
      "14302\n",
      "14303\n",
      "14304\n",
      "14305\n",
      "14306\n",
      "14307\n",
      "14308\n",
      "14309\n",
      "14310\n",
      "14311\n",
      "14312\n",
      "14313\n",
      "14314\n",
      "14315\n",
      "14316\n",
      "14317\n",
      "14318\n",
      "14319\n",
      "14320\n",
      "14321\n",
      "14322\n",
      "14323\n",
      "14324\n",
      "14325\n",
      "14326\n",
      "14327\n",
      "14328\n",
      "14329\n",
      "14330\n",
      "14331\n",
      "14332\n",
      "14333\n",
      "14334\n",
      "14335\n",
      "14400\n",
      "14401\n",
      "14402\n",
      "14403\n",
      "14404\n",
      "14405\n",
      "14406\n",
      "14407\n",
      "14408\n",
      "14409\n",
      "14410\n",
      "14411\n",
      "14412\n",
      "14413\n",
      "14414\n",
      "14415\n",
      "14656\n",
      "14657\n",
      "14658\n",
      "14659\n",
      "14660\n",
      "14661\n",
      "14662\n",
      "14663\n",
      "14664\n",
      "14665\n",
      "14666\n",
      "14667\n",
      "14668\n",
      "14669\n",
      "14670\n",
      "14671\n",
      "14800\n",
      "14801\n",
      "14802\n",
      "14803\n",
      "14804\n",
      "14805\n",
      "14806\n",
      "14807\n",
      "14808\n",
      "14809\n",
      "14810\n",
      "14811\n",
      "14812\n",
      "14813\n",
      "14814\n",
      "14815\n",
      "14816\n",
      "14817\n",
      "14818\n",
      "14819\n",
      "14820\n",
      "14821\n",
      "14822\n",
      "14823\n",
      "14824\n",
      "14825\n",
      "14826\n",
      "14827\n",
      "14828\n",
      "14829\n",
      "14830\n",
      "14831\n",
      "14832\n",
      "14833\n",
      "14834\n",
      "14835\n",
      "14836\n",
      "14837\n",
      "14838\n",
      "14839\n",
      "14840\n",
      "14841\n",
      "14842\n",
      "14843\n",
      "14844\n",
      "14845\n",
      "14846\n",
      "14847\n",
      "14992\n",
      "14993\n",
      "14994\n",
      "14995\n",
      "14996\n",
      "14997\n",
      "14998\n",
      "14999\n",
      "15000\n",
      "15001\n",
      "15002\n",
      "15003\n",
      "15004\n",
      "15005\n",
      "15006\n",
      "15007\n",
      "15008\n",
      "15009\n",
      "15010\n",
      "15011\n",
      "15012\n",
      "15013\n",
      "15014\n",
      "15015\n",
      "15016\n",
      "15017\n",
      "15018\n",
      "15019\n",
      "15020\n",
      "15021\n",
      "15022\n",
      "15023\n",
      "15024\n",
      "15025\n",
      "15026\n",
      "15027\n",
      "15028\n",
      "15029\n",
      "15030\n",
      "15031\n",
      "15032\n",
      "15033\n",
      "15034\n",
      "15035\n",
      "15036\n",
      "15037\n",
      "15038\n",
      "15039\n",
      "15360\n",
      "15361\n",
      "15362\n",
      "15363\n",
      "15364\n",
      "15365\n",
      "15366\n",
      "15367\n",
      "15368\n",
      "15369\n",
      "15370\n",
      "15371\n",
      "15372\n",
      "15373\n",
      "15374\n",
      "15375\n",
      "15376\n",
      "15377\n",
      "15378\n",
      "15379\n",
      "15380\n",
      "15381\n",
      "15382\n",
      "15383\n",
      "15384\n",
      "15385\n",
      "15386\n",
      "15387\n",
      "15388\n",
      "15389\n",
      "15390\n",
      "15391\n",
      "15392\n",
      "15393\n",
      "15394\n",
      "15395\n",
      "15396\n",
      "15397\n",
      "15398\n",
      "15399\n",
      "15400\n",
      "15401\n",
      "15402\n",
      "15403\n",
      "15404\n",
      "15405\n",
      "15406\n",
      "15407\n",
      "15456\n",
      "15457\n",
      "15458\n",
      "15459\n",
      "15460\n",
      "15461\n",
      "15462\n",
      "15463\n",
      "15464\n",
      "15465\n",
      "15466\n",
      "15467\n",
      "15468\n",
      "15469\n",
      "15470\n",
      "15471\n",
      "15472\n",
      "15473\n",
      "15474\n",
      "15475\n",
      "15476\n",
      "15477\n",
      "15478\n",
      "15479\n",
      "15480\n",
      "15481\n",
      "15482\n",
      "15483\n",
      "15484\n",
      "15485\n",
      "15486\n",
      "15487\n",
      "15488\n",
      "15489\n",
      "15490\n",
      "15491\n",
      "15492\n",
      "15493\n",
      "15494\n",
      "15495\n",
      "15496\n",
      "15497\n",
      "15498\n",
      "15499\n",
      "15500\n",
      "15501\n",
      "15502\n",
      "15503\n",
      "15568\n",
      "15569\n",
      "15570\n",
      "15571\n",
      "15572\n",
      "15573\n",
      "15574\n",
      "15575\n",
      "15576\n",
      "15577\n",
      "15578\n",
      "15579\n",
      "15580\n",
      "15581\n",
      "15582\n",
      "15583\n",
      "15824\n",
      "15825\n",
      "15826\n",
      "15827\n",
      "15828\n",
      "15829\n",
      "15830\n",
      "15831\n",
      "15832\n",
      "15833\n",
      "15834\n",
      "15835\n",
      "15836\n",
      "15837\n",
      "15838\n",
      "15839\n",
      "15840\n",
      "15841\n",
      "15842\n",
      "15843\n",
      "15844\n",
      "15845\n",
      "15846\n",
      "15847\n",
      "15848\n",
      "15849\n",
      "15850\n",
      "15851\n",
      "15852\n",
      "15853\n",
      "15854\n",
      "15855\n",
      "15872\n",
      "15873\n",
      "15874\n",
      "15875\n",
      "15876\n",
      "15877\n",
      "15878\n",
      "15879\n",
      "15880\n",
      "15881\n",
      "15882\n",
      "15883\n",
      "15884\n",
      "15885\n",
      "15886\n",
      "15887\n",
      "15968\n",
      "15969\n",
      "15970\n",
      "15971\n",
      "15972\n",
      "15973\n",
      "15974\n",
      "15975\n",
      "15976\n",
      "15977\n",
      "15978\n",
      "15979\n",
      "15980\n",
      "15981\n",
      "15982\n",
      "15983\n"
     ]
    }
   ],
   "source": [
    "for index,row in MRI_PET.iterrows():\n",
    "    if (row[\"Group\"] == 2):\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "category id: [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import GradCAM, show_cam_on_image\n",
    "import numpy as np\n",
    "target_layers = [model.ECA.layer4]\n",
    "\n",
    "\n",
    "# load image\n",
    "\n",
    "\n",
    "MRI_img = torch.from_numpy(MRI_PET.iloc[353]['MRI_img_array'].transpose(2,0,1)).float()\n",
    "PET_img = torch.from_numpy(MRI_PET.iloc[353]['PET_img_array'].transpose(2,0,1)).float()\n",
    "\n",
    "print(MRI_PET.iloc[353]['Group'])\n",
    "\n",
    "# [N, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "# expand batch dimension\n",
    "input_tensor_MRI = torch.unsqueeze(MRI_img, dim=0).cuda()\n",
    "input_tensor_PET = torch.unsqueeze(PET_img, dim=0).cuda()\n",
    "\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "target_category = 2\n",
    "grayscale_cam = cam(MRI=input_tensor_MRI, PET =input_tensor_PET)\n",
    "\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(MRI_PET.iloc[1221]['MRI_img_array'].astype(dtype=np.float32) / 255.,\n",
    "                                      grayscale_cam,\n",
    "                                      use_rgb=True)\n",
    "\n",
    "plt.imshow(visualization)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization1 = show_cam_on_image(MRI_PET.iloc[985]['PET_img_array'].astype(dtype=np.float32) / 255.,\n",
    "                                      grayscale_cam,\n",
    "                                      use_rgb=True)\n",
    "\n",
    "plt.imshow(visualization1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization2 = show_cam_on_image(MRI_PET.iloc[1]['PET_img_array'].astype(dtype=np.float32) / 255.,\n",
    "                                      grayscale_cam,\n",
    "                                      use_rgb=True)\n",
    "\n",
    "plt.imshow(visualization2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "def confusion_matrix(preds, labels, conf_matrix):\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[p, t] += 1\n",
    "    return conf_matrix\n",
    "conf_matrix = torch.zeros(3, 3)\n",
    "for mri,pet,group in test_loader:\n",
    "    output = model(mri.to(DEVICE),pet.to(DEVICE))\n",
    "    conf_matrix = confusion_matrix(output, group, conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1561.,  262.,  229.],\n",
       "        [ 129.,  589.,   31.],\n",
       "        [  54.,   13.,  348.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制混淆矩阵\n",
    "import matplotlib.pyplot as plt\n",
    "Emotion_kinds=3#这个数值是具体的分类数，大家可以自行修改\n",
    "labels = ['MCI', 'NC', 'AD']#每种类别的标签\n",
    " \n",
    "# 显示数据\n",
    "plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    " \n",
    "# 在图中标注数量/概率信息\n",
    "thresh = conf_matrix.max() / 2  #数值颜色阈值，如果数值超过这个，就颜色加深。\n",
    "for x in range(Emotion_kinds):\n",
    "    for y in range(Emotion_kinds):\n",
    "        # 注意这里的matrix[y, x]不是matrix[x, y]\n",
    "        info = int(conf_matrix[y, x])\n",
    "        plt.text(x, y, info,\n",
    "                 verticalalignment='center',\n",
    "                 horizontalalignment='center',\n",
    "                 color=\"white\" if info > thresh else \"black\")\n",
    "                  \n",
    "plt.tight_layout()#保证图不重叠\n",
    "plt.yticks(range(Emotion_kinds), labels)\n",
    "plt.xticks(range(Emotion_kinds), labels,rotation=45)#X轴字体倾斜45°\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py372')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e0e02a671462d396ecc20af6161881d6338b1e899f7d69a261eb7f1f955476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
